# model_optimizer.py
import torch
import json
import argparse
from transformers import DistilBertForSequenceClassification
import os


script_dir = os.path.dirname(os.path.abspath(__file__))  # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•
base_dir = os.path.dirname(script_dir)  # è·å–ModelTrainCodeç›®å½•
dataset_dir = os.path.join(base_dir, "Dataset")  # è·å–Datasetç›®å½•
model_path = os.path.join(dataset_dir, "privacy_detection_model.pth")
map_path = os.path.join(dataset_dir, "label_map.json")
model_save_dir = os.path.join(base_dir, "pretrained_models")
optimize_model_dir = os.path.join(dataset_dir, "optimized_privacy_model.onnx")


def optimize_model(
    input_model_path=model_path,
    output_onnx_path=optimize_model_dir,
    pruning_amount=0.2,
    label_map_path=map_path,
    max_len=128,
):
    """
    æ¨¡å‹ä¼˜åŒ–æµæ°´çº¿
    å‚æ•°ï¼š
        input_model_path: åŸå§‹PyTorchæ¨¡å‹è·¯å¾„
        output_onnx_path: ä¼˜åŒ–åONNXæ¨¡å‹ä¿å­˜è·¯å¾„
        pruning_amount: å‰ªææ¯”ä¾‹ (0-1)
        label_map_path: æ ‡ç­¾æ˜ å°„æ–‡ä»¶è·¯å¾„
        max_len: æ¨¡å‹æœ€å¤§è¾“å…¥é•¿åº¦
    """
    # åŠ è½½æ ‡ç­¾æ˜ å°„
    with open(label_map_path) as f:
        label_map = json.load(f)
        num_labels = len(label_map)

    # 1. åŠ è½½åŸå§‹æ¨¡å‹
    print("ğŸ”„ åŠ è½½åŸå§‹æ¨¡å‹...")
    model = DistilBertForSequenceClassification.from_pretrained(
        model_save_dir, num_labels=num_labels
    )
    model.load_state_dict(
        torch.load(input_model_path, map_location=torch.device("cpu"))
    )
    model.eval()

    # 2. åŠ¨æ€é‡åŒ–
    print("âš¡ æ‰§è¡ŒåŠ¨æ€é‡åŒ–...")
    quantized_model = torch.quantization.quantize_dynamic(
        model, {torch.nn.Linear}, dtype=torch.qint8
    )

    # 3. æ¨¡å‹å‰ªæ
    print("âœ‚ï¸ æ‰§è¡Œå…¨å±€å‰ªæ (æ¯”ä¾‹: {:.0%})...".format(pruning_amount))
    parameters_to_prune = [
        (module, "weight")
        for module in quantized_model.modules()
        if isinstance(module, torch.nn.Linear)
    ]

    torch.nn.utils.prune.global_unstructured(
        parameters_to_prune,
        pruning_method=torch.nn.utils.prune.L1Unstructured,
        amount=pruning_amount,
    )

    # æ°¸ä¹…ç§»é™¤å‰ªææ©ç 
    for module, _ in parameters_to_prune:
        torch.nn.utils.prune.remove(module, "weight")

    # 4. å¯¼å‡ºONNX
    print("ğŸ“¤ å¯¼å‡ºONNXæ¨¡å‹...")
    dummy_input = (
        torch.randint(0, 10000, (1, max_len)),  # input_ids
        torch.ones(1, max_len),  # attention_mask
    )

    torch.onnx.export(
        quantized_model,
        dummy_input,
        output_onnx_path,
        input_names=["input_ids", "attention_mask"],
        output_names=["logits"],
        dynamic_axes={
            "input_ids": {0: "batch_size"},
            "attention_mask": {0: "batch_size"},
            "logits": {0: "batch_size"},
        },
        opset_version=13,
        verbose=False,
    )

    print(f"âœ… ä¼˜åŒ–å®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {output_onnx_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ¨¡å‹ä¼˜åŒ–å·¥å…·")
    parser.add_argument("--input", type=str, default=model_path, help="è¾“å…¥æ¨¡å‹è·¯å¾„")
    parser.add_argument(
        "--output",
        type=str,
        default=optimize_model_dir,
        help="è¾“å‡ºONNXè·¯å¾„",
    )
    parser.add_argument("--prune", type=float, default=0.2, help="å‰ªææ¯”ä¾‹ (0-1)")
    parser.add_argument("--max_len", type=int, default=128, help="æ¨¡å‹æœ€å¤§è¾“å…¥é•¿åº¦")

    args = parser.parse_args()

    optimize_model(
        input_model_path=args.input,
        output_onnx_path=args.output,
        pruning_amount=args.prune,
        max_len=args.max_len,
    )
